Um algoritmo ideal é aquele que entrega exatamente o resultado esperado da forma mais eficiente possível. Como estudamos em Ciência da Computação, o computador interpreta cada instrução em linguagem binária [[03 - Bytes]], transmitida por meio de pulsos elétricos entre os componentes da placa. Por isso, ao desenvolver algoritmos, devemos sempre priorizar a **otimização**, reduzindo o uso desnecessário de recursos de hardware e buscando o máximo de **performance e velocidade** na execução.

## Big o 

a notação BIG O é o método padrão para medir eficiência de algoritmos, a notação big o descreve quão bem um algoritmo escala conforme o tamanho de sua entrada (N) aumenta.
